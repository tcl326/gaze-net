# -*- coding: utf-8 -*-
"""gaze-net.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xiH7xOldq99KgnVDsWuT9GR9mnsU0ZXa
"""

import os
import sys
import tensorflow as tf
import numpy as np
from keras.models import Model
from keras.layers import Input, Dense, LSTM, Lambda
from keras.engine.topology import Input
from keras import optimizers
from keras.utils.np_utils import to_categorical
from keras.models import Sequential, load_model
from keras.layers import Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout
from keras.layers import Activation, BatchNormalization, MaxPooling2D
import time,argparse
import math
from keras.utils import plot_model
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.image import ImageDataGenerator
from keras.engine.topology import Input
# from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot
from keras.utils import plot_model
from keras import backend as K
K.set_image_dim_ordering('tf')
import keras.callbacks

import gazenetGenerator2 as gaze_gen

# global param
dataset_path = '../gaze-net/gaze_dataset'
learning_rate = 0.0001
time_steps = 32
num_classes = 6
batch_size = 4
time_skip = 2
origin_image_size = 360    # size of the origin image before the cropWithGaze
img_size = 128    # size of the input image for network
num_channel = 3
steps_per_epoch=5
epochs=1
validation_step=20
total_num_epoch = 40


class GazeNet():
    def __init__(self,learning_rate,time_steps,num_classes,batch_size):
        self.learning_rate = learning_rate
        self.time_steps = time_steps
        self.num_classes = num_classes
        self.batch_size = batch_size
        self.kernel_size = 15
        self.kernel_num = 256
        self.gaussian_sigma = 1
        self.gaussian_weight = self.create_gaussian_weight()
        self.model = self.create_model()
    def create_gaussian_weight(self):
        kernel_size = self.kernel_size    #same with the shape of the layer before flatten
        kernel_num = self.kernel_num
        r = (kernel_size - 1) // 2
        sigma_2 = float(self.gaussian_sigma * self.gaussian_sigma)
        pi = 3.1415926
        ratio = 1 / (2*pi*sigma_2)
        kernel = np.zeros((kernel_size, kernel_size))
        for i in range(-r, r+1):
            for j in range(-r, r+1):
                tmp = math.exp(-(i*i+j*j)/(2*sigma_2))
                kernel[i+r][j+r] = round(tmp, 3)
        kernel *= ratio
        kernel = np.expand_dims(kernel, axis=2)
        kernel = np.tile(kernel, (1,1,kernel_num))
        # print(kernel.shape)
        return kernel
    def create_model(self):
        model = Sequential()
        def input_reshape(input):
            return tf.reshape(input, [self.batch_size*self.time_steps,128,128,3])

        model.add(Lambda(input_reshape, input_shape=(self.time_steps,128,128,3,)))
        #block 1
        model.add(Conv2D(96,(5,5),strides = (2,2),
                            padding = 'valid',
                            activation = 'relu'))
        model.add(BatchNormalization())
        model.add(MaxPooling2D(pool_size = (2,2)))

        #block 2
        model.add(Conv2D(256,(3,3),padding = 'same'))
        model.add(BatchNormalization())
        model.add(Activation('relu'))

        model.add(Conv2D(256,(3,3),padding = 'same'))
        model.add(BatchNormalization())
        model.add(Activation('relu'))

        model.add(Conv2D(256,(3,3),padding = 'same'))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(MaxPooling2D(2,2))

        def multiply_constant(input):
            for i in range(self.batch_size*self.time_steps):
                tmp = tf.multiply(tf.cast(input[i], tf.float32), tf.cast(self.gaussian_weight, tf.float32))
                tmp = tf.expand_dims(tmp, 0)
                if i == 0:
                    res = tmp
                else:
                    res = tf.concat([res, tmp], 0)
            res = tf.reshape(res,[self.batch_size,self.time_steps,
                                  self.kernel_size,self.kernel_size,self.kernel_num])
            res = tf.reshape(res,[self.batch_size,self.time_steps,
                                  self.kernel_size*self.kernel_size*self.kernel_num])
            return res

        model.add(Lambda(multiply_constant))

        def mean_value(input):
            return tf.reduce_mean(input,1)

        model.add(LSTM(128,return_sequences = True))
        model.add(LSTM(6,return_sequences = True))
        model.add(Lambda(mean_value))
        def classify(input):
            return tf.nn.softmax(input)
        model.add(Lambda(classify))
        adam = optimizers.Adam(lr = self.learning_rate)
        model.compile(loss='categorical_crossentropy', optimizer='adam')
        print(model.summary())

        return model

    def save_model_weights(self,save_path):
		# Helper function to save your model / weights.

        self.model.save_weights(save_path)
        # self.model.save(save_path)

        # return suffix
import keras.callbacks
class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = []

    def on_batch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))
from keras.callbacks import History


def parse_arguments():
	parser = argparse.ArgumentParser(description='Deep Q Network Argument Parser')
	parser.add_argument('--env',dest='env',type=str)
	parser.add_argument('--render',dest='render',type=int,default=0)
	parser.add_argument('--train',dest='train',type=int,default=1)

	return parser.parse_args()

def main(args):
    # generate model
    args = parse_arguments()
    gaze_net = GazeNet(learning_rate,time_steps,num_classes,batch_size)
    model = gaze_net.model
    #     plot_model(model, to_file='model.png')
    print("generate model!")
    if args.train == 1:
    # generatr generator
        for i in range(total_num_epoch):
            save_path = 'model2/'+str(i) + '/'
            if not os.path.exists(save_path):
                os.makedirs(save_path)

            trainGenerator = gaze_gen.GazeDataGenerator(validation_split=0.2)
            train_data = trainGenerator.flow_from_directory(dataset_path, subset='training',time_steps=time_steps,
                                                            batch_size=batch_size, crop=False,
                                                            gaussian_std=0.01, time_skip=time_skip, crop_with_gaze=True,
                                                           crop_with_gaze_size=128)
            val_data = trainGenerator.flow_from_directory(dataset_path, subset='validation', time_steps=time_steps,
                                                          batch_size=batch_size, crop=False,
                                                            gaussian_std=0.01, time_skip=time_skip, crop_with_gaze=True,
                                                           crop_with_gaze_size=128)
            # [img_seq, gaze_seq], output = next(trainGeneratorgDirectory)
            print("fetch data!")

            # start training
            # checkpointsString = "models/" + 'weights.{epoch:02d}-{val_loss:.2f}.hdf5'

            # callbacks = gaze_net.save_model_weights(checkpointsString)
            # history = History()
            # checkpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True)

            hist = model.fit_generator(train_data, steps_per_epoch=steps_per_epoch, epochs=epochs,
                            validation_data=val_data, validation_steps=validation_step, shuffle=False)
            print("finished training!")
            print(hist.history)
            file = open(save_path + 'losses.txt','a')
            file.writelines(["%s\n" % loss  for loss in hist.history.values()])
            if i%10 == 0:
                model.save_weights( save_path + 'weights.hdf5')
    else:

        model.load_weights('model/190/'+'weights.hdf5', by_name=False)
        testGenerator = gaze_gen.GazeDataGenerator(validation_split = 0.2)
        test_data = testGenerator.flow_from_directory(dataset_path, subset='training',time_steps=time_steps,
                                                        batch_size=batch_size, crop=False,
                                                        gaussian_std=0.01, time_skip=time_skip, crop_with_gaze=True,
                                                        crop_with_gaze_size=128)
        predicted_labels = model.predict_generator(test_data,steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=1)
        print(predicted_labels)
if __name__ == '__main__':
    main(sys.argv)
